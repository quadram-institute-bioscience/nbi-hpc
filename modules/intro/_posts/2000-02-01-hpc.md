---
title: "Why an HPC?"
---

This training assumes you are already familiar with using the Linux Command Line Interface (CLI).  
So why use an HPC instead of a single machine (that can be your local computer, a physical server or a Virtual Machine)?

A single server — no matter how powerful — has finite resources: a fixed number of CPU cores, limited RAM, and storage that can become a bottleneck. When you run multiple intensive jobs, they compete for these resources, slowing everything down. Worse, if a job crashes the system or hogs all memory, *everyone* suffers. 

Moreover a server might have long periods of underutilisation and when multiple users want to use it it might be insufficient. *How do they coordinate to use it?*

An HPC (High-Performance Computing) cluster changes the game. 
Instead of one machine, you have **many nodes** (individual servers) working together, orchestrated by a job scheduler. 
This gives you:

- **Scalability**: need 80 cores for a highly parallelisable task? Request them. Need 1 Tb RAM for a *de novo* assembly? Available.
- **Parallelism**: run hundreds of jobs simultaneously across different nodes (e.g. perform the same task on many samples, each as a separate process running in parallel)
- **Fair sharing**: the scheduler ensures resources are distributed fairly among users
- **Reliability**: your job runs on dedicated resources, isolated from others

Think of it as moving from a single kitchen where everyone cooks at once (chaos!) to a well-organised restaurant with multiple stations and a head chef coordinating everything.

## What is Slurm?


When you have a cluster with dozens (or hundreds) of nodes and many users wanting to run jobs, you need someone — or rather, *something* — to keep order. That's where **Slurm** comes in.

Slurm (Simple Linux Utility for Resource Management) is a **job scheduler**: it decides *when* and *where* your jobs run. You don't log into a compute node and start your analysis directly. Instead, you submit a job request to Slurm, specifying what you need (CPUs, memory, time), and Slurm queues it, finds available resources, and launches it for you.

This brings several advantages:

- **Fair queuing**: jobs are prioritised based on rules set by the admins — no one can monopolise the cluster
- **Resource allocation**: Slurm ensures your job gets *exactly* what you asked for, isolated from other users
- **Job tracking**: you can monitor, cancel, or check the status of your jobs at any time
- **Efficiency**: the scheduler fills gaps in resource usage, maximising the cluster's throughput

In practice, you'll interact with Slurm through a handful of commands:

| Command   | Purpose                              |
|-----------|--------------------------------------|
| `sbatch`  | Submit a batch job script            |
| `squeue`  | View the job queue                   |
| `scancel` | Cancel a job                         |
| `sinfo`   | Check node/partition status          |
| `srun`    | Run a command interactively on a node|

Think of Slurm as the air traffic controller of your HPC: it keeps everything running smoothly, prevents collisions, and makes sure everyone gets their turn on the runway.

## NBI HPC

 
![NBI Slurm]({{ site.baseurl }}/{% link img/slurm.png %})

Note how most of the nodes are not connected to the internet.