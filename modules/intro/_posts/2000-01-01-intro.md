---
title: Introduction
---

## Why an HPC?

This training assumes you are already familiar with using the Linux Command Line Interface (CLI).  
So why use an HPC instead of a single machine (that can be your local computer, a physical server or a Virtual Machine)?

A single server — no matter how powerful — has finite resources: a fixed number of CPU cores, limited RAM, and storage that can become a bottleneck. When you run multiple intensive jobs, they compete for these resources, slowing everything down. Worse, if a job crashes the system or hogs all memory, *everyone* suffers. 

Moreover a server might have long periods of underutilisation and when multiple users want to use it it might be insufficient. *How do they coordinate to use it?*

An HPC (High-Performance Computing) cluster changes the game. 
Instead of one machine, you have **many nodes** (individual servers) working together, orchestrated by a job scheduler. 
This gives you:

- **Scalability**: need 80 cores for a highly parallelisable task? Request them. Need 1 Tb RAM for a *de novo* assembly? Available.
- **Parallelism**: run hundreds of jobs simultaneously across different nodes (e.g. perform the same task on many samples, each as a separate process running in parallel)
- **Fair sharing**: the scheduler ensures resources are distributed fairly among users
- **Reliability**: your job runs on dedicated resources, isolated from others

Think of it as moving from a single kitchen where everyone cooks at once (chaos!) to a well-organised restaurant with multiple stations and a head chef coordinating everything.

## What we will do

1. We will learn how to use the NBI HPC and to submit jobs with the appropriate resources
2. We will install **micromamba** to manage our own packages
3. We will understand what **Singularity** containers are, and how to use existing singularity packages
4. We will see how to create our own Singularity containers to ensure reproducibility of our pipelines

 